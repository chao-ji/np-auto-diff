{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataflow graph representing a simple 4-layer ConvNet (Conv-Conv-FC-READOUT) is built by wiring the nodes (i.e. Operations) defined in `autodiff`. The ConvNet is the same as in [this tensorflow demo](https://www.tensorflow.org/get_started/mnist/pros). Dropout is applied on the 1st FC layer and is optimized using Adam algorithm.\n",
    "\n",
    "97% accuracy is achieved by training for only 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from autodiff import *\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "batch = 50\n",
    "iterations = 500 \n",
    "\n",
    "sess = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the terminal (i.e. leaf) nodes of the graph. Note **`X`**, **`Y`** and **`P`** (images, labels, and a scalar representing the dropout probability) must be supplied with values from a **`feed_dict`**. **`W_CONV1`**, **`B_CONV1`**, **`W_CONV2`**, **`B_CONV2`**, **`W_FC1`**, **`B_FC1`**, **`W_FC2`**, and **`B_FC2`** are trainable variables whose gradients are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = PlaceholderOp((batch, 28, 28, 1), sess, False)\n",
    "Y_ = PlaceholderOp((batch, 10), sess, False)\n",
    "P = PlaceholderOp((), sess, False)\n",
    "W_CONV1 = PlaceholderOp((5, 5, 1, 32), sess)\n",
    "B_CONV1 = PlaceholderOp((32,), sess)\n",
    "W_CONV2 = PlaceholderOp((5, 5, 32, 64), sess)\n",
    "B_CONV2 = PlaceholderOp((64,), sess)\n",
    "W_FC1 = PlaceholderOp((7 * 7 * 64, 1024), sess)\n",
    "B_FC1 = PlaceholderOp((1024,), sess)\n",
    "W_FC2 = PlaceholderOp((1024, 10), sess)\n",
    "B_FC2 = PlaceholderOp((10,), sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the graph. **`Y_CONV`** has shape `[batch, 10]` representing the logits for inference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONV1 = Conv2dOp(X, W_CONV1, [1, 1], \"SAME\", sess)\n",
    "CONV1_B = BiasAddOp(CONV1, B_CONV1, sess)\n",
    "H_CONV1 = ReluOp(CONV1_B, sess)\n",
    "H_POOL1 = MaxPool2dOp(H_CONV1, [2, 2], [2, 2], \"SAME\", sess)\n",
    "CONV2 = Conv2dOp(H_POOL1, W_CONV2, [1, 1], \"SAME\", sess)\n",
    "CONV2_B = BiasAddOp(CONV2, B_CONV2, sess)\n",
    "H_CONV2 = ReluOp(CONV2_B, sess)\n",
    "H_POOL2 = MaxPool2dOp(H_CONV2, [2, 2], [2, 2], \"SAME\", sess)\n",
    "H_POOL2_FLAT = ReshapeOp(H_POOL2, (batch, 7 * 7 * 64), sess)\n",
    "FC1 = MatMulOp(H_POOL2_FLAT, W_FC1, sess)\n",
    "FC1_B = BiasAddOp(FC1, B_FC1, sess)\n",
    "H_FC1 = ReluOp(FC1_B, sess)\n",
    "H_FC1_DROP = DropoutOp(H_FC1, P, sess)\n",
    "FC2 = MatMulOp(H_FC1_DROP, W_FC2, sess)\n",
    "Y_CONV = BiasAddOp(FC2, B_FC2, sess)\n",
    "SOFTMAX = SoftmaxCrossEntropyWithLogitsOp(Y_, Y_CONV, sess)\n",
    "CROSS_ENTROPY = ReduceMeanOp(SOFTMAX, 0, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, train accuracy: 0.120000, loss: 14.898926\n",
      "iteration: 50, train accuracy: 0.780000, loss: 0.614848\n",
      "iteration: 100, train accuracy: 0.840000, loss: 0.537072\n",
      "iteration: 150, train accuracy: 0.960000, loss: 0.168370\n",
      "iteration: 200, train accuracy: 0.860000, loss: 0.371053\n",
      "iteration: 250, train accuracy: 0.960000, loss: 0.057592\n",
      "iteration: 300, train accuracy: 0.980000, loss: 0.068481\n",
      "iteration: 350, train accuracy: 0.940000, loss: 0.144575\n",
      "iteration: 400, train accuracy: 0.960000, loss: 0.163229\n",
      "iteration: 450, train accuracy: 0.940000, loss: 0.189658\n"
     ]
    }
   ],
   "source": [
    "# Define initial values \n",
    "w_conv1 = np.random.normal(scale=0.1, size=W_CONV1.shape)\n",
    "b_conv1 = np.ones(B_CONV1.shape) * 0.1\n",
    "w_conv2 = np.random.normal(scale=0.1, size=W_CONV2.shape)\n",
    "b_conv2 = np.ones(B_CONV2.shape) * 0.1\n",
    "w_fc1 = np.random.normal(scale=0.1, size=W_FC1.shape)\n",
    "b_fc1 = np.ones(B_FC1.shape) * 0.1\n",
    "w_fc2 = np.random.normal(scale=0.1, size=W_FC2.shape)\n",
    "b_fc2 = np.ones(B_FC2.shape) * 0.1\n",
    "\n",
    "feed_dict = { W_CONV1: w_conv1,\n",
    "              B_CONV1: b_conv1,\n",
    "              W_CONV2: w_conv2,\n",
    "              B_CONV2: b_conv2,\n",
    "              W_FC1: w_fc1,\n",
    "              B_FC1: b_fc1,\n",
    "              W_FC2: w_fc2,\n",
    "              B_FC2: b_fc2}\n",
    "\n",
    "# Define parameters of Adam algorithm\n",
    "params = {\"alpha\":  1e-3,\n",
    "          \"beta1\":  .9,\n",
    "          \"beta2\":  .999,\n",
    "          \"epsilon\":  1e-8,\n",
    "          \"t\":  0,\n",
    "          \"m\":  { W_CONV1: np.zeros_like(w_conv1),\n",
    "                  B_CONV1: np.zeros_like(b_conv1),\n",
    "                  W_CONV2: np.zeros_like(w_conv2),\n",
    "                  B_CONV2: np.zeros_like(b_conv2),\n",
    "                  W_FC1: np.zeros_like(w_fc1),\n",
    "                  B_FC1: np.zeros_like(b_fc1),\n",
    "                  W_FC2: np.zeros_like(w_fc2),\n",
    "                  B_FC2: np.zeros_like(b_fc2)},\n",
    "          \"v\":  { W_CONV1: np.zeros_like(w_conv1),\n",
    "                  B_CONV1: np.zeros_like(b_conv1),\n",
    "                  W_CONV2: np.zeros_like(w_conv2),\n",
    "                  B_CONV2: np.zeros_like(b_conv2),\n",
    "                  W_FC1: np.zeros_like(w_fc1),\n",
    "                  B_FC1: np.zeros_like(b_fc1),\n",
    "                  W_FC2: np.zeros_like(w_fc2),\n",
    "                  B_FC2: np.zeros_like(b_fc2)}}\n",
    "\n",
    "# Train for `iterations` iterations\n",
    "for i in range(iterations):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch)\n",
    "\n",
    "    feed_dict[X] = batch_xs.reshape((batch, 28, 28, 1))\n",
    "    feed_dict[Y_] = batch_ys\n",
    "    feed_dict[P] = .5\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        Y_CONV_val = sess.eval_tensor(Y_CONV, feed_dict)\n",
    "        CROSS_ENTROPY_val = sess.eval_tensor(CROSS_ENTROPY, feed_dict)\n",
    "        print \"iteration: %d, train accuracy: %f, loss: %f\" % (i, np.mean(np.argmax(Y_CONV_val, axis=1) ==\n",
    "                                                                      np.argmax(batch_ys, axis=1)), \n",
    "                                                           CROSS_ENTROPY_val)\n",
    "\n",
    "    sess.adam_update(params, CROSS_ENTROPY, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9738\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([])\n",
    "y_true = np.array([])\n",
    "for i in range(0, mnist.test.images.shape[0], batch):\n",
    "    feed_dict[X] = mnist.test.images[i : i + batch].reshape((batch, 28, 28, 1))\n",
    "    feed_dict[P] = 1.\n",
    "    Y_CONV_val = sess.eval_tensor(Y_CONV, feed_dict)\n",
    "    y_pred = np.append(y_pred, np.argmax(Y_CONV_val, axis=1))\n",
    "    y_true = np.append(y_true, np.argmax(mnist.test.labels[i : i + batch], axis=1))\n",
    "\n",
    "test_accuracy = np.mean(y_pred == y_true)\n",
    "print \"accuracy =\", test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
