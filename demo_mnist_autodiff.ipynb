{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo of an implementation of multiplayer perceptron (aka artificial neural network) with 0 hidden layer. Reverse-mode auto differetiation ([1](http://cs231n.github.io/optimization-2/), [2](https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)) is used to compute the derivative of the loss function with respect to the parameters `W` and `B`.\n",
    "\n",
    "The neural network was trained and evaluated on the MNIST hand-written digit dataset\n",
    "\n",
    "The logits $S$ is defined as\n",
    "$$S = X\\cdot W + B$$, \n",
    "\n",
    "where $X$ is $n$-by-$p$, $W$ is $p$-by-$k$, and $b$ is $1$-by-$k$. In the case of MNIST, $p$ is 784 (28 pixels wide, 28 pixels high), $k$ is 10 (10 digits), $n$ is the number of training examples.\n",
    "\n",
    "So $S$ is $n$-by-$k$, and the predicted labels are the maximum value of each row of $S$. The loss function is defined as the avearge cross-entropy between true class label and predicted class label(i.e. applying softmax on each row of $S$, see [1](https://www.tensorflow.org/get_started/mnist/beginners))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from autodiff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "sess = Session()\n",
    "\n",
    "batch_size = 10000\n",
    "learning_rate = 1e-1\n",
    "reg = 1e-3 # regulariztion coefficient\n",
    "\n",
    "W_val = np.random.normal(scale=0.01, size=(784, 10))\n",
    "B_val = np.random.normal(scale=0.01, size=(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stage:\n",
      "iteration: 0, loss: 2.290462, train accuracy: 0.114600\n",
      "iteration: 100, loss: 0.621227, train accuracy: 0.859100\n",
      "iteration: 200, loss: 0.504227, train accuracy: 0.876500\n",
      "iteration: 300, loss: 0.444214, train accuracy: 0.886800\n",
      "iteration: 400, loss: 0.428894, train accuracy: 0.888000\n",
      "iteration: 500, loss: 0.412143, train accuracy: 0.890400\n",
      "iteration: 600, loss: 0.402179, train accuracy: 0.899600\n",
      "iteration: 700, loss: 0.382540, train accuracy: 0.900700\n",
      "iteration: 800, loss: 0.379738, train accuracy: 0.904400\n",
      "iteration: 900, loss: 0.383835, train accuracy: 0.900900\n"
     ]
    }
   ],
   "source": [
    "print \"Training stage:\"\n",
    "for _ in xrange(1000):\n",
    "\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    X = PlaceholderOp([batch_size, 784], sess)\n",
    "    W = PlaceholderOp([784, 10], sess)\n",
    "    I = PlaceholderOp([batch_size, 1], sess)\n",
    "    B = PlaceholderOp([1, 10], sess)\n",
    "    S = AddOp(MulOp(X, W, sess), MulOp(I, B, sess), sess)\n",
    "\n",
    "    H = SoftmaxCrossEntropyWithLogitsOp(S, np.where(batch_ys)[1], sess)\n",
    "\n",
    "    F = AddOp(H, RegMatOp(W, reg, sess), sess) # add regularization term on `W`\n",
    "\n",
    "    feed_dict = {X: batch_xs,\n",
    "        W: W_val,\n",
    "        I: np.ones((batch_size, 1)),\n",
    "        B: B_val}\n",
    "\n",
    "    if _ % 100 == 0:\n",
    "        loss = F.eval(feed_dict)\n",
    "        S_val = S.eval(feed_dict)\n",
    "        print  \"iteration: %d, loss: %f, train accuracy: %f\" % (_, loss, np.mean(np.argmax(S_val, axis=1) == np.argmax(batch_ys, axis=1)))\n",
    "    H.parent_total = 1\n",
    "    H.deriv(feed_dict, 1.) # propagate derivative dH/dH = 1. backwards \n",
    "    W_val += -learning_rate * sess.derivs[id(W)]\n",
    "    B_val += -learning_rate * sess.derivs[id(B)]\n",
    "\n",
    "    sess.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test stage:\n",
      "test set size: 10000, test accuracy: 0.910900\n"
     ]
    }
   ],
   "source": [
    "print \"Test stage:\"\n",
    "feed_dict = {X: mnist.test.images,\n",
    "        W: W_val,\n",
    "        I: np.ones((batch_size, 1)),\n",
    "        B: B_val}\n",
    "\n",
    "S_val = S.eval(feed_dict)\n",
    "print \"test set size: %d, test accuracy: %f\" % (mnist.test.images.shape[0], np.mean(np.argmax(S_val, axis=1) == np.argmax(mnist.test.labels, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
